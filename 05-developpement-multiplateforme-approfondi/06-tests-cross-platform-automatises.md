üîù Retour au [Sommaire](/SOMMAIRE.md)

# Tests cross-platform automatis√©s dans FreePascal/Lazarus

## Introduction

Lorsque vous d√©veloppez une application qui doit fonctionner sur Windows et Linux/Ubuntu, il est essentiel de s'assurer qu'elle se comporte correctement sur chaque plateforme. Les tests automatis√©s vous permettent de v√©rifier rapidement que votre code fonctionne partout, sans avoir √† tester manuellement sur chaque syst√®me. Ce tutoriel vous guidera dans la mise en place d'un syst√®me de tests efficace et portable.

## Comprendre les d√©fis des tests multi-plateformes

### Pourquoi les tests multi-plateformes sont diff√©rents

Votre application peut se comporter diff√©remment selon le syst√®me d'exploitation √† cause de :

- **Chemins de fichiers** : `C:\Users\Jean\Documents` (Windows) vs `/home/jean/Documents` (Linux)
- **Fins de ligne** : CRLF sous Windows, LF sous Linux
- **Sensibilit√© √† la casse** : Windows ignore la casse, Linux la respecte
- **Permissions** : Syst√®me tr√®s diff√©rent entre Windows et Linux
- **Comportement des API** : Les fonctions syst√®me peuvent r√©agir diff√©remment
- **Interface graphique** : Les widgets peuvent avoir des comportements vari√©s

### Types de tests n√©cessaires

1. **Tests unitaires** : V√©rifient des fonctions individuelles
2. **Tests d'int√©gration** : V√©rifient l'interaction entre composants
3. **Tests d'interface** : V√©rifient le comportement de l'interface graphique
4. **Tests de compatibilit√©** : V√©rifient les sp√©cificit√©s de chaque OS

## Configuration de FPCUnit

FPCUnit est le framework de tests int√©gr√© √† FreePascal, similaire √† JUnit pour Java ou NUnit pour .NET.

### Installation et configuration

FPCUnit est inclus avec FreePascal/Lazarus. Pour cr√©er un projet de tests :

1. Dans Lazarus : **Fichier** ‚Üí **Nouveau** ‚Üí **Projet** ‚Üí **Application de tests FPCUnit**
2. Choisissez l'interface (console ou graphique)
3. Lazarus cr√©e automatiquement la structure de base

### Structure d'un projet de tests

```
MonProjet/
‚îú‚îÄ‚îÄ src/                      # Code source de l'application
‚îÇ   ‚îú‚îÄ‚îÄ monunite.pas
‚îÇ   ‚îî‚îÄ‚îÄ utils.pas
‚îú‚îÄ‚îÄ tests/                    # Tests automatis√©s
‚îÇ   ‚îú‚îÄ‚îÄ testmonunite.pas
‚îÇ   ‚îú‚îÄ‚îÄ testutils.pas
‚îÇ   ‚îî‚îÄ‚îÄ testsuite.pas        # Suite de tests principale
‚îî‚îÄ‚îÄ MonProjet.lpi            # Projet principal
```

## Cr√©ation de tests unitaires de base

### Structure d'une classe de test

```pascal
unit TestMonUnite;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, FPCUnit, TestRegistry,
  MonUnite; // L'unit√© √† tester

type
  TTestMonUnite = class(TTestCase)
  protected
    procedure SetUp; override;
    procedure TearDown; override;
  published
    // Les m√©thodes de test doivent √™tre dans la section published
    procedure TestAddition;
    procedure TestDivision;
    procedure TestCheminFichier;
  end;

implementation

procedure TTestMonUnite.SetUp;
begin
  // Code ex√©cut√© avant chaque test
  // Initialiser les variables, cr√©er des objets, etc.
end;

procedure TTestMonUnite.TearDown;
begin
  // Code ex√©cut√© apr√®s chaque test
  // Nettoyer, lib√©rer la m√©moire, supprimer les fichiers temporaires
end;

procedure TTestMonUnite.TestAddition;
begin
  // Test simple
  AssertEquals('2 + 2 devrait √©galer 4', 4, Addition(2, 2));
  AssertEquals('Nombres n√©gatifs', -5, Addition(-2, -3));
end;

procedure TTestMonUnite.TestDivision;
begin
  // Test avec v√©rification d'exception
  AssertEquals('Division normale', 2.5, Division(5, 2), 0.001);

  // V√©rifier qu'une exception est lev√©e
  AssertException('Division par z√©ro', EDivByZero,
    @TestDivisionParZero);
end;

procedure TTestMonUnite.TestCheminFichier;
var
  Chemin: string;
begin
  // Test avec comportement diff√©rent selon l'OS
  {$IFDEF WINDOWS}
    Chemin := 'C:\Temp\test.txt';
    AssertEquals('Chemin Windows', 'C:\Temp\test.txt',
                 NormaliserChemin(Chemin));
  {$ENDIF}

  {$IFDEF UNIX}
    Chemin := '/tmp/test.txt';
    AssertEquals('Chemin Unix', '/tmp/test.txt',
                 NormaliserChemin(Chemin));
  {$ENDIF}
end;

initialization
  RegisterTest(TTestMonUnite);

end.
```

### M√©thodes d'assertion disponibles

FPCUnit fournit de nombreuses m√©thodes pour v√©rifier vos r√©sultats :

```pascal
// Assertions de base
AssertTrue('Message', Condition);
AssertFalse('Message', Condition);
AssertEquals('Message', Attendu, Obtenu);
AssertNull('Message', Objet);
AssertNotNull('Message', Objet);
AssertSame('Message', Objet1, Objet2);

// Assertions pour les nombres flottants (avec tol√©rance)
AssertEquals('Message', 3.14, Pi, 0.01);

// Assertions pour les cha√Ænes
AssertEquals('Cha√Ænes identiques', 'Hello', MaFonction());

// V√©rification d'exceptions
AssertException('Message', ExceptionClass, @MethodeQuiLeve);
```

## Tests sp√©cifiques √† chaque plateforme

### Utilisation de la compilation conditionnelle

```pascal
unit TestPlateforme;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, FPCUnit, TestRegistry;

type
  TTestPlateforme = class(TTestCase)
  published
    procedure TestCheminsSysteme;
    procedure TestPermissions;
    procedure TestProcessus;
  end;

implementation

procedure TTestPlateforme.TestCheminsSysteme;
var
  CheminTemp: string;
  CheminHome: string;
begin
  {$IFDEF WINDOWS}
    CheminTemp := GetEnvironmentVariable('TEMP');
    AssertTrue('TEMP existe sous Windows', CheminTemp <> '');
    AssertTrue('Format chemin Windows', Pos(':\', CheminTemp) > 0);

    CheminHome := GetEnvironmentVariable('USERPROFILE');
    AssertTrue('USERPROFILE existe', CheminHome <> '');
  {$ENDIF}

  {$IFDEF UNIX}
    CheminTemp := '/tmp';
    AssertTrue('Dossier /tmp existe', DirectoryExists(CheminTemp));

    CheminHome := GetEnvironmentVariable('HOME');
    AssertTrue('HOME existe sous Unix', CheminHome <> '');
    AssertTrue('Format chemin Unix', CheminHome[1] = '/');
  {$ENDIF}
end;

procedure TTestPlateforme.TestPermissions;
var
  NomFichier: string;
  Fichier: TextFile;
begin
  // Cr√©er un fichier temporaire
  NomFichier := GetTempFileName;
  AssignFile(Fichier, NomFichier);
  Rewrite(Fichier);
  WriteLn(Fichier, 'Test');
  CloseFile(Fichier);

  try
    {$IFDEF WINDOWS}
      // Sous Windows, tester les attributs
      AssertTrue('Fichier lisible', FileIsReadable(NomFichier));
      FileSetAttr(NomFichier, faReadOnly);
      AssertFalse('Fichier non modifiable', FileIsWritable(NomFichier));
      FileSetAttr(NomFichier, 0); // Retirer readonly
    {$ENDIF}

    {$IFDEF UNIX}
      // Sous Unix, tester les permissions chmod
      AssertTrue('Fichier lisible', FileIsReadable(NomFichier));
      FpChmod(NomFichier, &444); // Lecture seule
      AssertFalse('Fichier non modifiable', FileIsWritable(NomFichier));
      FpChmod(NomFichier, &644); // Permissions normales
    {$ENDIF}
  finally
    DeleteFile(NomFichier);
  end;
end;

procedure TTestPlateforme.TestProcessus;
var
  Output: string;
begin
  {$IFDEF WINDOWS}
    // Tester une commande Windows
    AssertTrue('Commande dir fonctionne',
               ExecuterCommande('cmd /c dir', Output));
    AssertTrue('Output non vide', Length(Output) > 0);
  {$ENDIF}

  {$IFDEF UNIX}
    // Tester une commande Unix
    AssertTrue('Commande ls fonctionne',
               ExecuterCommande('ls', Output));
    AssertTrue('Output non vide', Length(Output) > 0);
  {$ENDIF}
end;

initialization
  RegisterTest(TTestPlateforme);

end.
```

## Tests d'interface graphique

### Tests de formulaires et composants

```pascal
unit TestInterface;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, FPCUnit, TestRegistry, Forms, Controls,
  FormPrincipal; // Le formulaire √† tester

type
  TTestInterface = class(TTestCase)
  private
    FForm: TFormPrincipal;
  protected
    procedure SetUp; override;
    procedure TearDown; override;
  published
    procedure TestCreationFormulaire;
    procedure TestBoutons;
    procedure TestMenus;
    procedure TestRedimensionnement;
  end;

implementation

procedure TTestInterface.SetUp;
begin
  // Cr√©er le formulaire pour les tests
  Application.Initialize;
  FForm := TFormPrincipal.Create(nil);
end;

procedure TTestInterface.TearDown;
begin
  // Lib√©rer le formulaire
  FForm.Free;
  FForm := nil;
end;

procedure TTestInterface.TestCreationFormulaire;
begin
  AssertNotNull('Formulaire cr√©√©', FForm);
  AssertEquals('Titre correct', 'Mon Application', FForm.Caption);

  // V√©rifier la taille selon la plateforme
  {$IFDEF WINDOWS}
    AssertTrue('Largeur Windows', FForm.Width >= 800);
  {$ENDIF}

  {$IFDEF UNIX}
    // Linux peut avoir des contraintes diff√©rentes
    AssertTrue('Largeur Linux', FForm.Width >= 640);
  {$ENDIF}
end;

procedure TTestInterface.TestBoutons;
begin
  AssertNotNull('Bouton OK existe', FForm.BtnOK);
  AssertTrue('Bouton OK visible', FForm.BtnOK.Visible);
  AssertTrue('Bouton OK activ√©', FForm.BtnOK.Enabled);

  // Simuler un clic
  FForm.BtnOK.Click;

  // V√©rifier le r√©sultat
  AssertEquals('Action effectu√©e', 'OK', FForm.DernierAction);
end;

procedure TTestInterface.TestMenus;
begin
  AssertNotNull('Menu principal existe', FForm.MainMenu);
  AssertTrue('Menu Fichier existe', FForm.MenuFichier.Count > 0);

  // Tester un √©l√©ment de menu
  AssertNotNull('Menu Ouvrir existe', FForm.MenuOuvrir);
  AssertTrue('Raccourci correct', FForm.MenuOuvrir.ShortCut <> 0);

  // V√©rifier les raccourcis selon l'OS
  {$IFDEF WINDOWS}
    // Ctrl+O sous Windows
    AssertEquals('Raccourci Windows', 'Ctrl+O',
                 ShortCutToText(FForm.MenuOuvrir.ShortCut));
  {$ENDIF}
end;

procedure TTestInterface.TestRedimensionnement;
var
  AncienneLargeur: Integer;
begin
  AncienneLargeur := FForm.Width;

  // Redimensionner
  FForm.Width := 1024;
  Application.ProcessMessages; // Traiter les √©v√©nements

  AssertEquals('Nouvelle largeur', 1024, FForm.Width);

  // V√©rifier que les composants s'adaptent
  if FForm.Panel1.Align = alClient then
    AssertEquals('Panel redimensionn√©', FForm.Width,
                 FForm.Panel1.Width);
end;

initialization
  RegisterTest(TTestInterface);

end.
```

## Organisation des suites de tests

### Cr√©ation d'une suite de tests principale

```pascal
unit TestSuite;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, FPCUnit, TestRegistry,
  // Importer tous vos modules de tests
  TestMonUnite,
  TestPlateforme,
  TestInterface,
  TestBaseDonnees,
  TestReseau;

type
  TTestSuitePrincipale = class(TTestSuite)
  public
    class function Suite: TTestSuite;
  end;

implementation

class function TTestSuitePrincipale.Suite: TTestSuite;
var
  TestSuite: TTestSuite;
begin
  TestSuite := TTestSuite.Create('Tests Complets Application');

  // Ajouter les cat√©gories de tests
  TestSuite.AddTest(TTestSuite.Create('Tests Unitaires'));
  TestSuite.AddTest(TTestMonUnite);

  TestSuite.AddTest(TTestSuite.Create('Tests Plateforme'));
  TestSuite.AddTest(TTestPlateforme);

  // Tests conditionnels selon l'OS
  {$IFDEF WINDOWS}
    TestSuite.AddTest(TTestSuite.Create('Tests Windows'));
    TestSuite.AddTest(TTestWindows);
  {$ENDIF}

  {$IFDEF UNIX}
    TestSuite.AddTest(TTestSuite.Create('Tests Unix'));
    TestSuite.AddTest(TTestUnix);
  {$ENDIF}

  // Tests d'interface seulement si pas en mode console
  if not IsConsole then
  begin
    TestSuite.AddTest(TTestSuite.Create('Tests Interface'));
    TestSuite.AddTest(TTestInterface);
  end;

  Result := TestSuite;
end;

initialization
  RegisterTest(TTestSuitePrincipale.Suite);

end.
```

## Ex√©cution des tests

### Application console de tests

```pascal
program TestsConsole;

{$mode objfpc}{$H+}

uses
  Classes, SysUtils, FPCUnit, TestRegistry,
  ConsoleTestRunner,
  TestSuite; // Votre suite de tests

var
  Application: TTestRunner;

begin
  // Configurer les options
  DefaultFormat := fPlain; // ou fXML pour sortie XML
  DefaultRunAllTests := True;

  Application := TTestRunner.Create(nil);
  try
    Application.Initialize;
    Application.Title := 'Tests Multi-plateformes';
    Application.Run;
  finally
    Application.Free;
  end;
end.
```

### Application graphique de tests

```pascal
program TestsGUI;

{$mode objfpc}{$H+}

uses
  Interfaces, Forms, GuiTestRunner,
  TestSuite; // Votre suite de tests

{$R *.res}

begin
  Application.Initialize;
  Application.CreateForm(TGuiTestRunner, TestRunner);
  Application.Run;
end.
```

## Automatisation avec scripts

### Script de tests pour Windows (test.bat)

```batch
@echo off
echo ========================================
echo Tests automatises Windows
echo ========================================

REM Compiler les tests
fpc -Mobjfpc -Sh TestsConsole.pas

if %errorlevel% neq 0 (
    echo Erreur de compilation!
    exit /b 1
)

REM Executer les tests
TestsConsole.exe --format=xml --all > resultats_windows.xml

if %errorlevel% neq 0 (
    echo Des tests ont echoue!
    type resultats_windows.xml
    exit /b 1
)

echo Tous les tests sont passes!
exit /b 0
```

### Script de tests pour Linux (test.sh)

```bash
#!/bin/bash

echo "========================================"
echo "Tests automatis√©s Linux/Ubuntu"
echo "========================================"

# Compiler les tests
fpc -Mobjfpc -Sh TestsConsole.pas

if [ $? -ne 0 ]; then
    echo "Erreur de compilation!"
    exit 1
fi

# Ex√©cuter les tests
./TestsConsole --format=xml --all > resultats_linux.xml

if [ $? -ne 0 ]; then
    echo "Des tests ont √©chou√©!"
    cat resultats_linux.xml
    exit 1
fi

echo "Tous les tests sont pass√©s!"
exit 0
```

## Int√©gration continue (CI)

### Configuration GitHub Actions

Cr√©ez `.github/workflows/tests.yml` :

```yaml
name: Tests Multi-plateformes

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test-windows:
    runs-on: windows-latest
    steps:
    - uses: actions/checkout@v2

    - name: Installer FreePascal
      run: |
        choco install freepascal

    - name: Compiler les tests
      run: |
        fpc -Mobjfpc -Sh tests/TestsConsole.pas

    - name: Ex√©cuter les tests
      run: |
        cd tests
        TestsConsole.exe --format=plain --all

    - name: Upload r√©sultats
      uses: actions/upload-artifact@v2
      with:
        name: resultats-windows
        path: tests/*.xml

  test-ubuntu:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2

    - name: Installer FreePascal
      run: |
        sudo apt-get update
        sudo apt-get install -y fpc

    - name: Compiler les tests
      run: |
        fpc -Mobjfpc -Sh tests/TestsConsole.pas

    - name: Ex√©cuter les tests
      run: |
        cd tests
        ./TestsConsole --format=plain --all

    - name: Upload r√©sultats
      uses: actions/upload-artifact@v2
      with:
        name: resultats-ubuntu
        path: tests/*.xml
```

## Gestion des donn√©es de test

### Fichiers de test portables

```pascal
unit TestData;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils;

type
  TTestDataManager = class
  private
    FDataPath: string;
    function GetTestDataPath: string;
  public
    constructor Create;
    function GetTestFile(const AFileName: string): string;
    procedure CreateTestFile(const AFileName, AContent: string);
    procedure CleanupTestFiles;
  end;

implementation

constructor TTestDataManager.Create;
begin
  FDataPath := GetTestDataPath;
  ForceDirectories(FDataPath);
end;

function TTestDataManager.GetTestDataPath: string;
begin
  // Utiliser un dossier temporaire selon l'OS
  {$IFDEF WINDOWS}
    Result := GetEnvironmentVariable('TEMP') + '\TestData\';
  {$ENDIF}

  {$IFDEF UNIX}
    Result := '/tmp/TestData/';
  {$ENDIF}

  // S'assurer que le dossier existe
  if not DirectoryExists(Result) then
    CreateDir(Result);
end;

function TTestDataManager.GetTestFile(const AFileName: string): string;
begin
  Result := FDataPath + AFileName;
end;

procedure TTestDataManager.CreateTestFile(const AFileName, AContent: string);
var
  F: TextFile;
  FullPath: string;
begin
  FullPath := GetTestFile(AFileName);
  AssignFile(F, FullPath);
  Rewrite(F);
  try
    Write(F, AContent);
  finally
    CloseFile(F);
  end;
end;

procedure TTestDataManager.CleanupTestFiles;
var
  SearchRec: TSearchRec;
begin
  // Supprimer tous les fichiers de test
  if FindFirst(FDataPath + '*.*', faAnyFile, SearchRec) = 0 then
  begin
    repeat
      if (SearchRec.Attr and faDirectory) = 0 then
        DeleteFile(FDataPath + SearchRec.Name);
    until FindNext(SearchRec) <> 0;
    FindClose(SearchRec);
  end;

  // Supprimer le dossier
  RemoveDir(FDataPath);
end;

end.
```

## Tests de performance cross-platform

### Mesure des performances

```pascal
unit TestPerformance;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, FPCUnit, TestRegistry, DateUtils;

type
  TTestPerformance = class(TTestCase)
  private
    FStartTime: TDateTime;
    procedure StartTimer;
    function StopTimer: Integer; // Retourne les millisecondes
  published
    procedure TestVitesseCalcul;
    procedure TestVitesseIO;
    procedure TestVitesseMemoire;
  end;

implementation

procedure TTestPerformance.StartTimer;
begin
  FStartTime := Now;
end;

function TTestPerformance.StopTimer: Integer;
begin
  Result := MilliSecondsBetween(Now, FStartTime);
end;

procedure TTestPerformance.TestVitesseCalcul;
var
  i, j: Integer;
  Temps: Integer;
  MaxTemps: Integer;
begin
  // D√©finir les limites selon l'OS
  {$IFDEF WINDOWS}
    MaxTemps := 1000; // 1 seconde max sur Windows
  {$ENDIF}

  {$IFDEF UNIX}
    MaxTemps := 800; // Linux g√©n√©ralement plus rapide
  {$ENDIF}

  StartTimer;

  // Calcul intensif
  for i := 1 to 1000000 do
    j := i * 2 + i div 3;

  Temps := StopTimer;

  AssertTrue(Format('Calcul en moins de %d ms (actuel: %d ms)',
             [MaxTemps, Temps]), Temps < MaxTemps);
end;

procedure TTestPerformance.TestVitesseIO;
var
  F: TextFile;
  i: Integer;
  NomFichier: string;
  Temps: Integer;
begin
  NomFichier := GetTempFileName;

  StartTimer;

  // √âcriture
  AssignFile(F, NomFichier);
  Rewrite(F);
  try
    for i := 1 to 10000 do
      WriteLn(F, 'Ligne de test num√©ro ', i);
  finally
    CloseFile(F);
  end;

  // Lecture
  AssignFile(F, NomFichier);
  Reset(F);
  try
    while not Eof(F) do
      ReadLn(F);
  finally
    CloseFile(F);
  end;

  Temps := StopTimer;
  DeleteFile(NomFichier);

  // Les performances IO varient beaucoup selon l'OS
  {$IFDEF WINDOWS}
    AssertTrue('IO Windows < 2000ms', Temps < 2000);
  {$ENDIF}

  {$IFDEF UNIX}
    AssertTrue('IO Unix < 1500ms', Temps < 1500);
  {$ENDIF}
end;

procedure TTestPerformance.TestVitesseMemoire;
var
  Liste: TStringList;
  i: Integer;
  Temps: Integer;
begin
  StartTimer;

  Liste := TStringList.Create;
  try
    // Allocation et manipulation m√©moire
    for i := 1 to 100000 do
      Liste.Add('√âl√©ment ' + IntToStr(i));

    Liste.Sort;
    Liste.Clear;
  finally
    Liste.Free;
  end;

  Temps := StopTimer;

  AssertTrue('Gestion m√©moire < 500ms', Temps < 500);
end;

initialization
  RegisterTest(TTestPerformance);

end.
```

## Rapports de tests

### G√©n√©ration de rapports HTML

```pascal
unit RapportTests;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, FPCUnit, TestReport;

type
  TRapportHTML = class
  private
    FResultats: TTestResult;
    FNomFichier: string;
  public
    constructor Create(AResultats: TTestResult;
                      const ANomFichier: string);
    procedure Generer;
  end;

implementation

constructor TRapportHTML.Create(AResultats: TTestResult;
                                const ANomFichier: string);
begin
  FResultats := AResultats;
  FNomFichier := ANomFichier;
end;

procedure TRapportHTML.Generer;
var
  F: TextFile;
  i: Integer;
begin
  AssignFile(F, FNomFichier);
  Rewrite(F);
  try
    WriteLn(F, '<html>');
    WriteLn(F, '<head>');
    WriteLn(F, '<title>Rapport de Tests Multi-plateformes</title>');
    WriteLn(F, '<style>');
    WriteLn(F, 'body { font-family: Arial, sans-serif; }');
    WriteLn(F, '.success { color: green; }');
    WriteLn(F, '.failure { color: red; }');
    WriteLn(F, '.info { background: #f0f0f0; padding: 10px; }');
    WriteLn(F, '</style>');
    WriteLn(F, '</head>');
    WriteLn(F, '<body>');

    WriteLn(F, '<h1>Rapport de Tests</h1>');

    // Informations syst√®me
    WriteLn(F, '<div class="info">');
    WriteLn(F, '<h2>Environnement de test</h2>');
    {$IFDEF WINDOWS}
      WriteLn(F, '<p>Syst√®me : Windows</p>');
    {$ENDIF}
    {$IFDEF UNIX}
      WriteLn(F, '<p>Syst√®me : Linux/Unix</p>');
    {$ENDIF}
    WriteLn(F, '<p>Date : ', DateTimeToStr(Now), '</p>');
    WriteLn(F, '</div>');

    // R√©sum√©
    WriteLn(F, '<h2>R√©sum√©</h2>');
    WriteLn(F, '<ul>');
    WriteLn(F, '<li>Tests ex√©cut√©s : ', FResultats.RunTests, '</li>');

    if FResultats.NumberOfFailures = 0 then
      WriteLn(F, '<li class="success">√âchecs : 0</li>')
    else
      WriteLn(F, '<li class="failure">√âchecs : ',
              FResultats.NumberOfFailures, '</li>');

    WriteLn(F, '<li>Erreurs : ', FResultats.NumberOfErrors, '</li>');
    WriteLn(F, '</ul>');

    // D√©tails des √©checs
    if FResultats.NumberOfFailures > 0 then
    begin
      WriteLn(F, '<h2>D√©tails des √©checs</h2>');
      WriteLn(F, '<ul>');
      for i := 0 to FResultats.Failures.Count - 1 do
      begin
        WriteLn(F, '<li class="failure">');
        WriteLn(F, FResultats.Failures[i].AsString);
        WriteLn(F, '</li>');
      end;
      WriteLn(F, '</ul>');
    end;

    WriteLn(F, '</body>');
    WriteLn(F, '</html>');
  finally
    CloseFile(F);
  end;
end;

end.
```

## Conseils et bonnes pratiques

### Organisation des tests

1. **Un fichier de test par unit√©** : Gardez une correspondance claire
2. **Noms explicites** : `TestCalculateurTaxe` plut√¥t que `Test1`
3. **Tests ind√©pendants** : Chaque test doit pouvoir s'ex√©cuter seul
4. **Nettoyage syst√©matique** : Utilisez `TearDown` pour nettoyer

### Tests efficaces

```pascal
procedure TMonTest.TestExempleComplet;
var
  Resultat: Double;
  FichierTest: string;
begin
  // Arrange (Pr√©parer)
  FichierTest := CreateTestFile('donnees.txt');

  try
    // Act (Agir)
    Resultat := CalculerDepuisFichier(FichierTest);

    // Assert (V√©rifier)
    AssertEquals('R√©sultat attendu', 42.0, Resultat, 0.01);
  finally
    // Cleanup (Nettoyer)
    DeleteFile(FichierTest);
  end;
end;
```

### Gestion des diff√©rences OS

```pascal
procedure TTestOS.TestComportementSpecifique;
begin
  {$IFDEF WINDOWS}
    RunTestWindows;
  {$ELSE}
    {$IFDEF UNIX}
      RunTestUnix;
    {$ELSE}
      Ignore('Plateforme non support√©e');
    {$ENDIF}
  {$ENDIF}
end;

procedure TTestOS.RunTestWindows;
begin
  // Tests sp√©cifiques Windows
  AssertTrue('Service Windows', WindowsServiceExists('Spooler'));
end;

procedure TTestOS.RunTestUnix;
begin
  // Tests sp√©cifiques Unix
  AssertTrue('Daemon Unix', UnixDaemonExists('cron'));
end;
```

## D√©bogage des tests qui √©chouent

### Techniques de diagnostic

```pascal
procedure TTestDebug.TestAvecDiagnostic;
var
  Valeur: Integer;
  Message: string;
begin
  // Ajouter des informations de contexte
  Valeur := CalculerQuelqueChose();

  // Message d√©taill√© en cas d'√©chec
  Message := Format('√âchec du calcul. Valeur obtenue: %d, ' +
                    'Valeur attendue: 100, OS: %s, ' +
                    'Version FPC: %s',
                    [Valeur,
                     {$IFDEF WINDOWS}'Windows'{$ELSE}'Unix'{$ENDIF},
                     {$I %FPCVERSION%}]);

  AssertEquals(Message, 100, Valeur);

  // Logging pour debug
  if IsDebugMode then
  begin
    WriteLn('DEBUG: Valeur calcul√©e = ', Valeur);
    WriteLn('DEBUG: M√©moire utilis√©e = ', GetHeapStatus.TotalAllocated);
  end;
end;
```

### Cr√©ation d'un syst√®me de logging pour les tests

```pascal
unit TestLogger;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils;

type
  TLogLevel = (llDebug, llInfo, llWarning, llError);

  TTestLogger = class
  private
    FLogFile: TextFile;
    FLogFileName: string;
    FEnabled: Boolean;
    FLogLevel: TLogLevel;
  public
    constructor Create(const AFileName: string);
    destructor Destroy; override;
    procedure Log(ALevel: TLogLevel; const AMessage: string);
    procedure LogPlatform(const AMessage: string);
    procedure LogMemory;
    procedure LogException(E: Exception);
    property Enabled: Boolean read FEnabled write FEnabled;
    property LogLevel: TLogLevel read FLogLevel write FLogLevel;
  end;

var
  TestLog: TTestLogger;

implementation

constructor TTestLogger.Create(const AFileName: string);
begin
  FLogFileName := AFileName;
  FEnabled := True;
  FLogLevel := llDebug;

  // Cr√©er le fichier de log
  AssignFile(FLogFile, FLogFileName);
  Rewrite(FLogFile);

  // En-t√™te du log
  WriteLn(FLogFile, '===========================================');
  WriteLn(FLogFile, 'Log de tests - ', DateTimeToStr(Now));
  WriteLn(FLogFile, 'Plateforme: ',
          {$IFDEF WINDOWS}'Windows'{$ENDIF}
          {$IFDEF UNIX}'Unix/Linux'{$ENDIF});
  WriteLn(FLogFile, 'Version FPC: ', {$I %FPCVERSION%});
  WriteLn(FLogFile, '===========================================');
  WriteLn(FLogFile);

  Flush(FLogFile);
end;

destructor TTestLogger.Destroy;
begin
  if FEnabled then
  begin
    WriteLn(FLogFile);
    WriteLn(FLogFile, 'Fin du log - ', DateTimeToStr(Now));
    CloseFile(FLogFile);
  end;
  inherited;
end;

procedure TTestLogger.Log(ALevel: TLogLevel; const AMessage: string);
const
  LevelStr: array[TLogLevel] of string =
    ('DEBUG', 'INFO', 'WARNING', 'ERROR');
begin
  if not FEnabled then Exit;
  if ALevel < FLogLevel then Exit;

  WriteLn(FLogFile, Format('[%s] %s - %s',
          [LevelStr[ALevel], TimeToStr(Now), AMessage]));
  Flush(FLogFile);
end;

procedure TTestLogger.LogPlatform(const AMessage: string);
begin
  {$IFDEF WINDOWS}
    Log(llInfo, '[Windows] ' + AMessage);
  {$ENDIF}

  {$IFDEF UNIX}
    Log(llInfo, '[Unix] ' + AMessage);
  {$ENDIF}
end;

procedure TTestLogger.LogMemory;
var
  HeapStatus: THeapStatus;
begin
  HeapStatus := GetHeapStatus;
  Log(llDebug, Format('M√©moire - Allou√©e: %d, Libre: %d, Total: %d',
      [HeapStatus.TotalAllocated,
       HeapStatus.TotalFree,
       HeapStatus.CurrHeapSize]));
end;

procedure TTestLogger.LogException(E: Exception);
begin
  Log(llError, 'Exception: ' + E.ClassName + ' - ' + E.Message);
  {$IFDEF WINDOWS}
    if E is EOSError then
      Log(llError, 'Code erreur Windows: ' + IntToStr(GetLastError));
  {$ENDIF}
end;

initialization
  TestLog := TTestLogger.Create('test_debug.log');

finalization
  TestLog.Free;

end.
```

### Utilisation du logger dans les tests

```pascal
unit TestAvecLog;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, FPCUnit, TestRegistry, TestLogger;

type
  TTestAvecLog = class(TTestCase)
  protected
    procedure SetUp; override;
    procedure TearDown; override;
  published
    procedure TestOperationComplexe;
  end;

implementation

procedure TTestAvecLog.SetUp;
begin
  TestLog.Log(llInfo, 'D√©but du test: ' + Self.TestName);
  TestLog.LogMemory;
end;

procedure TTestAvecLog.TearDown;
begin
  TestLog.LogMemory;
  TestLog.Log(llInfo, 'Fin du test: ' + Self.TestName);
end;

procedure TTestAvecLog.TestOperationComplexe;
var
  i: Integer;
  Liste: TStringList;
begin
  TestLog.Log(llDebug, 'Cr√©ation de la liste');
  Liste := TStringList.Create;
  try
    // Op√©ration qui pourrait varier selon l'OS
    for i := 1 to 1000 do
    begin
      Liste.Add('Item ' + IntToStr(i));
      if i mod 100 = 0 then
        TestLog.Log(llDebug, Format('%d items ajout√©s', [i]));
    end;

    TestLog.LogPlatform('Tri de la liste');
    Liste.Sort;

    AssertEquals('Nombre d''√©l√©ments', 1000, Liste.Count);
    TestLog.Log(llInfo, 'Test r√©ussi');

  except
    on E: Exception do
    begin
      TestLog.LogException(E);
      raise; // Relancer l'exception pour que le test √©choue
    end;
  end;

  Liste.Free;
end;

initialization
  RegisterTest(TTestAvecLog);

end.
```

## Mock Objects et isolation des tests

### Cr√©ation de mocks simples

```pascal
unit MockObjects;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils;

type
  // Interface pour le service r√©el
  IFileService = interface
    ['{A1B2C3D4-E5F6-7890-ABCD-EF1234567890}']
    function ReadFile(const AFileName: string): string;
    procedure WriteFile(const AFileName: string; const AContent: string);
    function FileExists(const AFileName: string): Boolean;
  end;

  // Mock pour les tests
  TMockFileService = class(TInterfacedObject, IFileService)
  private
    FFiles: TStringList;
    FReadCount: Integer;
    FWriteCount: Integer;
  public
    constructor Create;
    destructor Destroy; override;

    // Impl√©mentation de l'interface
    function ReadFile(const AFileName: string): string;
    procedure WriteFile(const AFileName: string; const AContent: string);
    function FileExists(const AFileName: string): Boolean;

    // M√©thodes de v√©rification pour les tests
    property ReadCount: Integer read FReadCount;
    property WriteCount: Integer read FWriteCount;
    procedure Reset;
    procedure SetFileContent(const AFileName, AContent: string);
  end;

  // Service r√©el (pour production)
  TRealFileService = class(TInterfacedObject, IFileService)
  public
    function ReadFile(const AFileName: string): string;
    procedure WriteFile(const AFileName: string; const AContent: string);
    function FileExists(const AFileName: string): Boolean;
  end;

implementation

{ TMockFileService }

constructor TMockFileService.Create;
begin
  FFiles := TStringList.Create;
  FReadCount := 0;
  FWriteCount := 0;
end;

destructor TMockFileService.Destroy;
begin
  FFiles.Free;
  inherited;
end;

function TMockFileService.ReadFile(const AFileName: string): string;
var
  Index: Integer;
begin
  Inc(FReadCount);
  Index := FFiles.IndexOfName(AFileName);
  if Index >= 0 then
    Result := FFiles.ValueFromIndex[Index]
  else
    raise Exception.Create('Fichier non trouv√©: ' + AFileName);
end;

procedure TMockFileService.WriteFile(const AFileName: string;
                                     const AContent: string);
begin
  Inc(FWriteCount);
  FFiles.Values[AFileName] := AContent;
end;

function TMockFileService.FileExists(const AFileName: string): Boolean;
begin
  Result := FFiles.IndexOfName(AFileName) >= 0;
end;

procedure TMockFileService.Reset;
begin
  FFiles.Clear;
  FReadCount := 0;
  FWriteCount := 0;
end;

procedure TMockFileService.SetFileContent(const AFileName, AContent: string);
begin
  FFiles.Values[AFileName] := AContent;
end;

{ TRealFileService }

function TRealFileService.ReadFile(const AFileName: string): string;
var
  F: TextFile;
  Line: string;
begin
  Result := '';
  if not FileExists(AFileName) then
    raise Exception.Create('Fichier non trouv√©: ' + AFileName);

  AssignFile(F, AFileName);
  Reset(F);
  try
    while not Eof(F) do
    begin
      ReadLn(F, Line);
      Result := Result + Line + LineEnding;
    end;
  finally
    CloseFile(F);
  end;
end;

procedure TRealFileService.WriteFile(const AFileName: string;
                                    const AContent: string);
var
  F: TextFile;
begin
  AssignFile(F, AFileName);
  Rewrite(F);
  try
    Write(F, AContent);
  finally
    CloseFile(F);
  end;
end;

function TRealFileService.FileExists(const AFileName: string): Boolean;
begin
  Result := SysUtils.FileExists(AFileName);
end;

end.
```

### Utilisation des mocks dans les tests

```pascal
unit TestAvecMocks;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, FPCUnit, TestRegistry, MockObjects;

type
  TProcesseurFichier = class
  private
    FFileService: IFileService;
  public
    constructor Create(AFileService: IFileService);
    function ProcesserFichier(const ANomFichier: string): string;
  end;

  TTestProcesseurFichier = class(TTestCase)
  private
    FMockService: TMockFileService;
    FProcesseur: TProcesseurFichier;
  protected
    procedure SetUp; override;
    procedure TearDown; override;
  published
    procedure TestProcesserFichierSimple;
    procedure TestProcesserFichierInexistant;
    procedure TestVerifierAppels;
  end;

implementation

{ TProcesseurFichier }

constructor TProcesseurFichier.Create(AFileService: IFileService);
begin
  FFileService := AFileService;
end;

function TProcesseurFichier.ProcesserFichier(const ANomFichier: string): string;
var
  Contenu: string;
begin
  if not FFileService.FileExists(ANomFichier) then
    Exit('Fichier introuvable');

  Contenu := FFileService.ReadFile(ANomFichier);

  // Traitement du contenu
  Result := UpperCase(Contenu);

  // Sauvegarder le r√©sultat
  FFileService.WriteFile(ANomFichier + '.processed', Result);
end;

{ TTestProcesseurFichier }

procedure TTestProcesseurFichier.SetUp;
begin
  FMockService := TMockFileService.Create;
  FProcesseur := TProcesseurFichier.Create(FMockService);
end;

procedure TTestProcesseurFichier.TearDown;
begin
  FProcesseur.Free;
  // FMockService est lib√©r√© automatiquement (interface)
end;

procedure TTestProcesseurFichier.TestProcesserFichierSimple;
var
  Resultat: string;
begin
  // Pr√©parer le mock
  FMockService.SetFileContent('test.txt', 'hello world');

  // Ex√©cuter
  Resultat := FProcesseur.ProcesserFichier('test.txt');

  // V√©rifier
  AssertEquals('Contenu transform√©', 'HELLO WORLD', Resultat);
  AssertTrue('Fichier de sortie cr√©√©',
             FMockService.FileExists('test.txt.processed'));
end;

procedure TTestProcesseurFichier.TestProcesserFichierInexistant;
var
  Resultat: string;
begin
  // Le fichier n'existe pas dans le mock
  Resultat := FProcesseur.ProcesserFichier('inexistant.txt');

  AssertEquals('Message d''erreur', 'Fichier introuvable', Resultat);
end;

procedure TTestProcesseurFichier.TestVerifierAppels;
begin
  FMockService.SetFileContent('data.txt', 'test');

  FProcesseur.ProcesserFichier('data.txt');

  // V√©rifier que les m√©thodes ont √©t√© appel√©es
  AssertEquals('Nombre de lectures', 1, FMockService.ReadCount);
  AssertEquals('Nombre d''√©critures', 1, FMockService.WriteCount);
end;

initialization
  RegisterTest(TTestProcesseurFichier);

end.
```

## Tests de r√©gression

### Syst√®me de snapshots

```pascal
unit TestSnapshots;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, FPCUnit, TestRegistry, fpjson, jsonparser;

type
  TSnapshotManager = class
  private
    FSnapshotDir: string;
    function GetSnapshotPath(const AName: string): string;
  public
    constructor Create;
    procedure SaveSnapshot(const AName: string; const AData: string);
    function LoadSnapshot(const AName: string): string;
    function CompareWithSnapshot(const AName, AActual: string): Boolean;
    procedure UpdateSnapshot(const AName: string; const ANewData: string);
  end;

  TTestRegression = class(TTestCase)
  private
    FSnapshots: TSnapshotManager;
  protected
    procedure SetUp; override;
    procedure TearDown; override;
  published
    procedure TestFormatageJSON;
    procedure TestCalculComplexe;
    procedure TestSortieHTML;
  end;

implementation

{ TSnapshotManager }

constructor TSnapshotManager.Create;
begin
  {$IFDEF WINDOWS}
    FSnapshotDir := ExtractFilePath(ParamStr(0)) + 'snapshots\';
  {$ELSE}
    FSnapshotDir := ExtractFilePath(ParamStr(0)) + 'snapshots/';
  {$ENDIF}

  ForceDirectories(FSnapshotDir);
end;

function TSnapshotManager.GetSnapshotPath(const AName: string): string;
begin
  Result := FSnapshotDir + AName + '.snapshot';
end;

procedure TSnapshotManager.SaveSnapshot(const AName: string;
                                        const AData: string);
var
  F: TextFile;
begin
  AssignFile(F, GetSnapshotPath(AName));
  Rewrite(F);
  try
    Write(F, AData);
  finally
    CloseFile(F);
  end;
end;

function TSnapshotManager.LoadSnapshot(const AName: string): string;
var
  F: TextFile;
  Line: string;
begin
  Result := '';
  if not FileExists(GetSnapshotPath(AName)) then
    Exit;

  AssignFile(F, GetSnapshotPath(AName));
  Reset(F);
  try
    while not Eof(F) do
    begin
      ReadLn(F, Line);
      if Result <> '' then
        Result := Result + LineEnding;
      Result := Result + Line;
    end;
  finally
    CloseFile(F);
  end;
end;

function TSnapshotManager.CompareWithSnapshot(const AName,
                                              AActual: string): Boolean;
var
  Expected: string;
begin
  Expected := LoadSnapshot(AName);

  // Si pas de snapshot, le cr√©er
  if Expected = '' then
  begin
    SaveSnapshot(AName, AActual);
    Result := True;
    WriteLn('Nouveau snapshot cr√©√©: ', AName);
  end
  else
    Result := Expected = AActual;
end;

procedure TSnapshotManager.UpdateSnapshot(const AName: string;
                                         const ANewData: string);
begin
  SaveSnapshot(AName, ANewData);
  WriteLn('Snapshot mis √† jour: ', AName);
end;

{ TTestRegression }

procedure TTestRegression.SetUp;
begin
  FSnapshots := TSnapshotManager.Create;
end;

procedure TTestRegression.TearDown;
begin
  FSnapshots.Free;
end;

procedure TTestRegression.TestFormatageJSON;
var
  JSON: TJSONObject;
  Output: string;
begin
  // Cr√©er un objet JSON
  JSON := TJSONObject.Create;
  try
    JSON.Add('nom', 'Test');
    JSON.Add('version', '1.0');
    JSON.Add('plateforme', {$IFDEF WINDOWS}'Windows'{$ELSE}'Unix'{$ENDIF});

    Output := JSON.FormatJSON;

    // Comparer avec le snapshot
    AssertTrue('JSON correspond au snapshot',
               FSnapshots.CompareWithSnapshot('json_output', Output));
  finally
    JSON.Free;
  end;
end;

procedure TTestRegression.TestCalculComplexe;
var
  Resultat: string;
  i: Integer;
begin
  // Calcul qui doit toujours donner le m√™me r√©sultat
  Resultat := '';
  for i := 1 to 10 do
    Resultat := Resultat + Format('Ligne %d: %d' + LineEnding,
                                  [i, i * i]);

  AssertTrue('Calcul correspond au snapshot',
             FSnapshots.CompareWithSnapshot('calcul_complexe', Resultat));
end;

procedure TTestRegression.TestSortieHTML;
var
  HTML: TStringList;
begin
  HTML := TStringList.Create;
  try
    HTML.Add('<html>');
    HTML.Add('<head><title>Test</title></head>');
    HTML.Add('<body>');
    HTML.Add('<h1>Plateforme: ' +
             {$IFDEF WINDOWS}'Windows'{$ELSE}'Unix'{$ENDIF} +
             '</h1>');
    HTML.Add('<p>Version: 1.0</p>');
    HTML.Add('</body>');
    HTML.Add('</html>');

    AssertTrue('HTML correspond au snapshot',
               FSnapshots.CompareWithSnapshot('sortie_html', HTML.Text));
  finally
    HTML.Free;
  end;
end;

initialization
  RegisterTest(TTestRegression);

end.
```

## Tests de compatibilit√© multi-versions

### Tests pour diff√©rentes versions d'OS

```pascal
unit TestCompatibilite;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, FPCUnit, TestRegistry
  {$IFDEF WINDOWS}, Windows{$ENDIF}
  {$IFDEF UNIX}, BaseUnix{$ENDIF};

type
  TTestCompatibilite = class(TTestCase)
  private
    function GetOSVersion: string;
    function IsWindows10OrNewer: Boolean;
    function IsUbuntu20OrNewer: Boolean;
  published
    procedure TestVersionOS;
    procedure TestFonctionnalitesModernes;
    procedure TestRetrocompatibilite;
  end;

implementation

function TTestCompatibilite.GetOSVersion: string;
{$IFDEF WINDOWS}
var
  OSVersionInfo: TOSVersionInfo;
begin
  OSVersionInfo.dwOSVersionInfoSize := SizeOf(TOSVersionInfo);
  if GetVersionEx(OSVersionInfo) then
    Result := Format('Windows %d.%d Build %d',
              [OSVersionInfo.dwMajorVersion,
               OSVersionInfo.dwMinorVersion,
               OSVersionInfo.dwBuildNumber])
  else
    Result := 'Windows (version inconnue)';
end;
{$ELSE}
var
  F: TextFile;
  Line: string;
begin
  Result := 'Unix/Linux';

  // Essayer de lire la version depuis /etc/os-release
  if FileExists('/etc/os-release') then
  begin
    AssignFile(F, '/etc/os-release');
    Reset(F);
    try
      while not Eof(F) do
      begin
        ReadLn(F, Line);
        if Pos('PRETTY_NAME=', Line) = 1 then
        begin
          Result := Copy(Line, 13, Length(Line) - 13);
          Result := StringReplace(Result, '"', '', [rfReplaceAll]);
          Break;
        end;
      end;
    finally
      CloseFile(F);
    end;
  end;
end;
{$ENDIF}

function TTestCompatibilite.IsWindows10OrNewer: Boolean;
{$IFDEF WINDOWS}
var
  OSVersionInfo: TOSVersionInfo;
begin
  OSVersionInfo.dwOSVersionInfoSize := SizeOf(TOSVersionInfo);
  if GetVersionEx(OSVersionInfo) then
    Result := OSVersionInfo.dwMajorVersion >= 10
  else
    Result := False;
end;
{$ELSE}
begin
  Result := False;
end;
{$ENDIF}

function TTestCompatibilite.IsUbuntu20OrNewer: Boolean;
{$IFDEF UNIX}
var
  F: TextFile;
  Line: string;
  Version: string;
begin
  Result := False;

  if FileExists('/etc/os-release') then
  begin
    AssignFile(F, '/etc/os-release');
    Reset(F);
    try
      while not Eof(F) do
      begin
        ReadLn(F, Line);
        if Pos('VERSION_ID=', Line) = 1 then
        begin
          Version := Copy(Line, 12, Length(Line) - 12);
          Version := StringReplace(Version, '"', '', [rfReplaceAll]);

          // V√©rifier si c'est Ubuntu 20.04 ou plus r√©cent
          if Pos('20', Version) = 1 then
            Result := True
          else if Pos('21', Version) = 1 then
            Result := True
          else if Pos('22', Version) = 1 then
            Result := True;

          Break;
        end;
      end;
    finally
      CloseFile(F);
    end;
  end;
end;
{$ELSE}
begin
  Result := False;
end;
{$ENDIF}

procedure TTestCompatibilite.TestVersionOS;
var
  Version: string;
begin
  Version := GetOSVersion;

  WriteLn('OS d√©tect√©: ', Version);

  {$IFDEF WINDOWS}
    AssertTrue('Windows d√©tect√©', Pos('Windows', Version) > 0);
  {$ENDIF}

  {$IFDEF UNIX}
    AssertTrue('Unix/Linux d√©tect√©',
               (Pos('Linux', Version) > 0) or (Pos('Ubuntu', Version) > 0));
  {$ENDIF}
end;

procedure TTestCompatibilite.TestFonctionnalitesModernes;
begin
  {$IFDEF WINDOWS}
    if IsWindows10OrNewer then
    begin
      // Tester les fonctionnalit√©s Windows 10+
      WriteLn('Windows 10+ d√©tect√© - Tests modernes activ√©s');
      // Par exemple, v√©rifier le support des notifications modernes
      AssertTrue('Support notifications Windows 10', True);
    end
    else
    begin
      WriteLn('Windows ancien - Tests de compatibilit√©');
      Ignore('Fonctionnalit√©s Windows 10+ non disponibles');
    end;
  {$ENDIF}

  {$IFDEF UNIX}
    if IsUbuntu20OrNewer then
    begin
      WriteLn('Ubuntu 20+ d√©tect√© - Tests modernes activ√©s');
      // Tester les fonctionnalit√©s Ubuntu 20+
      AssertTrue('Systemd disponible',
                 FileExists('/bin/systemctl'));
    end
    else
    begin
      WriteLn('Version Ubuntu ancienne');
    end;
  {$ENDIF}
end;

procedure TTestCompatibilite.TestRetrocompatibilite;
begin
  // Tester que les fonctionnalit√©s de base fonctionnent partout

  // Test de cr√©ation de fichier (doit marcher partout)
  AssertTrue('Cr√©ation fichier temporaire',
             Length(GetTempFileName) > 0);

  // Test des variables d'environnement basiques
  {$IFDEF WINDOWS}
    AssertTrue('Variable PATH existe',
               GetEnvironmentVariable('PATH') <> '');
  {$ENDIF}

  {$IFDEF UNIX}
    AssertTrue('Variable HOME existe',
               GetEnvironmentVariable('HOME') <> '');
  {$ENDIF}
end;

initialization
  RegisterTest(TTestCompatibilite);

end.
```

## Benchmarking et tests de charge

### Framework de benchmark portable

```pascal
unit TestBenchmark;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, FPCUnit, TestRegistry, DateUtils;

type
  TBenchmarkResult = record
    NomTest: string;
    Iterations: Integer;
    TempsMoyen: Double;  // en millisecondes
    TempsMin: Double;
    TempsMax: Double;
    EcartType: Double;
  end;

  TBenchmarkRunner = class
  private
    FResultats: array of TBenchmarkResult;
    function CalculerEcartType(const Temps: array of Double): Double;
  public
    function RunBenchmark(const ANom: string;
                         AProc: TProcedure;
                         AIterations: Integer = 1000): TBenchmarkResult;
    procedure SaveResults(const AFileName: string);
    procedure CompareWithBaseline(const ABaseline: string);
  end;

  TTestPerformanceCrossPlatform = class(TTestCase)
  private
    FBenchmark: TBenchmarkRunner;
    procedure BenchStringConcat;
    procedure BenchListSort;
    procedure BenchFileIO;
  protected
    procedure SetUp; override;
    procedure TearDown; override;
  published
    procedure TestPerformanceStrings;
    procedure TestPerformanceListes;
    procedure TestPerformanceIO;
  end;

implementation

{ TBenchmarkRunner }

function TBenchmarkRunner.CalculerEcartType(const Temps: array of Double): Double;
var
  Moyenne, Somme: Double;
  i: Integer;
begin
  if Length(Temps) = 0 then
    Exit(0);

  // Calculer la moyenne
  Somme := 0;
  for i := 0 to High(Temps) do
    Somme := Somme + Temps[i];
  Moyenne := Somme / Length(Temps);

  // Calculer l'√©cart-type
  Somme := 0;
  for i := 0 to High(Temps) do
    Somme := Somme + Sqr(Temps[i] - Moyenne);

  Result := Sqrt(Somme / Length(Temps));
end;

function TBenchmarkRunner.RunBenchmark(const ANom: string;
                                       AProc: TProcedure;
                                       AIterations: Integer): TBenchmarkResult;
var
  i, WarmUp: Integer;
  StartTime, EndTime: TDateTime;
  Temps: array of Double;
  TempTotal: Double;
begin
  Result.NomTest := ANom;
  Result.Iterations := AIterations;

  // Warm-up (10% des it√©rations)
  WarmUp := AIterations div 10;
  for i := 1 to WarmUp do
    AProc();

  // Mesures r√©elles
  SetLength(Temps, AIterations);
  for i := 0 to AIterations - 1 do
  begin
    StartTime := Now;
    AProc();
    EndTime := Now;
    Temps[i] := MilliSecondsBetween(EndTime, StartTime);
  end;

  // Calcul des statistiques
  Result.TempsMin := Temps[0];
  Result.TempsMax := Temps[0];
  TempTotal := 0;

  for i := 0 to High(Temps) do
  begin
    TempTotal := TempTotal + Temps[i];
    if Temps[i] < Result.TempsMin then
      Result.TempsMin := Temps[i];
    if Temps[i] > Result.TempsMax then
      Result.TempsMax := Temps[i];
  end;

  Result.TempsMoyen := TempTotal / AIterations;
  Result.EcartType := CalculerEcartType(Temps);

  // Ajouter aux r√©sultats
  SetLength(FResultats, Length(FResultats) + 1);
  FResultats[High(FResultats)] := Result;
end;

procedure TBenchmarkRunner.SaveResults(const AFileName: string);
var
  F: TextFile;
  i: Integer;
begin
  AssignFile(F, AFileName);
  Rewrite(F);
  try
    WriteLn(F, 'R√©sultats de Benchmark - ', DateTimeToStr(Now));
    WriteLn(F, 'Plateforme: ',
            {$IFDEF WINDOWS}'Windows'{$ELSE}'Linux/Unix'{$ENDIF});
    WriteLn(F, '================================================');
    WriteLn(F);

    for i := 0 to High(FResultats) do
    begin
      with FResultats[i] do
      begin
        WriteLn(F, 'Test: ', NomTest);
        WriteLn(F, '  It√©rations: ', Iterations);
        WriteLn(F, '  Temps moyen: ', TempsMoyen:0:3, ' ms');
        WriteLn(F, '  Temps min: ', TempsMin:0:3, ' ms');
        WriteLn(F, '  Temps max: ', TempsMax:0:3, ' ms');
        WriteLn(F, '  √âcart-type: ', EcartType:0:3, ' ms');
        WriteLn(F);
      end;
    end;

    // Format CSV pour analyse
    WriteLn(F);
    WriteLn(F, 'Format CSV:');
    WriteLn(F, 'Test;Iterations;Moyenne;Min;Max;EcartType');
    for i := 0 to High(FResultats) do
    begin
      with FResultats[i] do
        WriteLn(F, Format('%s;%d;%.3f;%.3f;%.3f;%.3f',
                [NomTest, Iterations, TempsMoyen,
                 TempsMin, TempsMax, EcartType]));
    end;
  finally
    CloseFile(F);
  end;
end;

procedure TBenchmarkRunner.CompareWithBaseline(const ABaseline: string);
var
  F: TextFile;
  Line: string;
  BaselineResults: array of TBenchmarkResult;
  i, j: Integer;
  Difference: Double;
begin
  if not FileExists(ABaseline) then
  begin
    WriteLn('Pas de baseline trouv√©e, sauvegarde actuelle comme baseline');
    SaveResults(ABaseline);
    Exit;
  end;

  // Charger les r√©sultats de r√©f√©rence
  // (Code simplifi√© - en r√©alit√©, il faudrait parser le CSV)

  WriteLn('Comparaison avec baseline:');
  WriteLn('==========================');

  for i := 0 to High(FResultats) do
  begin
    // Rechercher le test correspondant dans la baseline
    for j := 0 to High(BaselineResults) do
    begin
      if BaselineResults[j].NomTest = FResultats[i].NomTest then
      begin
        Difference := ((FResultats[i].TempsMoyen -
                       BaselineResults[j].TempsMoyen) /
                       BaselineResults[j].TempsMoyen) * 100;

        Write(FResultats[i].NomTest, ': ');
        if Difference > 10 then
          WriteLn('R√âGRESSION - ', Difference:0:1, '% plus lent')
        else if Difference < -10 then
          WriteLn('AM√âLIORATION - ', Abs(Difference):0:1, '% plus rapide')
        else
          WriteLn('Stable (', Difference:0:1, '%)');

        Break;
      end;
    end;
  end;
end;

{ TTestPerformanceCrossPlatform }

procedure TTestPerformanceCrossPlatform.SetUp;
begin
  FBenchmark := TBenchmarkRunner.Create;
end;

procedure TTestPerformanceCrossPlatform.TearDown;
var
  NomFichier: string;
begin
  // Sauvegarder les r√©sultats
  {$IFDEF WINDOWS}
    NomFichier := 'benchmark_windows.txt';
  {$ELSE}
    NomFichier := 'benchmark_linux.txt';
  {$ENDIF}

  FBenchmark.SaveResults(NomFichier);
  FBenchmark.CompareWithBaseline('benchmark_baseline.txt');

  FBenchmark.Free;
end;

procedure TTestPerformanceCrossPlatform.BenchStringConcat;
var
  S: string;
  i: Integer;
begin
  S := '';
  for i := 1 to 100 do
    S := S + IntToStr(i);
end;

procedure TTestPerformanceCrossPlatform.BenchListSort;
var
  Liste: TStringList;
  i: Integer;
begin
  Liste := TStringList.Create;
  try
    for i := 1 to 100 do
      Liste.Add(IntToStr(Random(1000)));
    Liste.Sort;
  finally
    Liste.Free;
  end;
end;

procedure TTestPerformanceCrossPlatform.BenchFileIO;
var
  NomFichier: string;
  F: TextFile;
  i: Integer;
begin
  NomFichier := GetTempFileName;

  AssignFile(F, NomFichier);
  Rewrite(F);
  try
    for i := 1 to 100 do
      WriteLn(F, 'Ligne ', i);
  finally
    CloseFile(F);
  end;

  DeleteFile(NomFichier);
end;

procedure TTestPerformanceCrossPlatform.TestPerformanceStrings;
var
  Result: TBenchmarkResult;
begin
  Result := FBenchmark.RunBenchmark('Concat√©nation strings',
                                    @BenchStringConcat, 100);

  // V√©rifier que les performances sont acceptables
  {$IFDEF WINDOWS}
    AssertTrue('Performance strings Windows', Result.TempsMoyen < 10);
  {$ELSE}
    AssertTrue('Performance strings Linux', Result.TempsMoyen < 8);
  {$ENDIF}
end;

procedure TTestPerformanceCrossPlatform.TestPerformanceListes;
var
  Result: TBenchmarkResult;
begin
  Result := FBenchmark.RunBenchmark('Tri de listes',
                                    @BenchListSort, 100);

  AssertTrue('Performance tri acceptable', Result.TempsMoyen < 20);
end;

procedure TTestPerformanceCrossPlatform.TestPerformanceIO;
var
  Result: TBenchmarkResult;
begin
  Result := FBenchmark.RunBenchmark('I/O fichiers',
                                    @BenchFileIO, 50);

  // L'I/O est g√©n√©ralement plus lent sur Windows
  {$IFDEF WINDOWS}
    AssertTrue('Performance I/O Windows', Result.TempsMoyen < 50);
  {$ELSE}
    AssertTrue('Performance I/O Linux', Result.TempsMoyen < 30);
  {$ENDIF}
end;

initialization
  RegisterTest(TTestPerformanceCrossPlatform);

end.
```

## Int√©gration avec Jenkins

### Configuration Jenkins pour tests multi-plateformes

Cr√©ez un fichier `Jenkinsfile` √† la racine de votre projet :

```groovy
pipeline {
    agent none

    stages {
        stage('Tests Multi-Plateformes') {
            parallel {
                stage('Tests Windows') {
                    agent { label 'windows' }
                    steps {
                        checkout scm

                        script {
                            echo 'Compilation sur Windows'
                            bat '''
                                fpc -Mobjfpc -Sh -B tests/TestsConsole.pas
                                if %errorlevel% neq 0 exit /b %errorlevel%
                            '''

                            echo 'Ex√©cution des tests Windows'
                            bat '''
                                cd tests
                                TestsConsole.exe --format=xml --all > results_windows.xml
                            '''
                        }
                    }
                    post {
                        always {
                            junit 'tests/results_windows.xml'
                            archiveArtifacts artifacts: 'tests/*.log',
                                           allowEmptyArchive: true
                        }
                    }
                }

                stage('Tests Linux') {
                    agent { label 'linux' }
                    steps {
                        checkout scm

                        script {
                            echo 'Compilation sur Linux'
                            sh '''
                                fpc -Mobjfpc -Sh -B tests/TestsConsole.pas
                                if [ $? -ne 0 ]; then exit 1; fi
                            '''

                            echo 'Ex√©cution des tests Linux'
                            sh '''
                                cd tests
                                ./TestsConsole --format=xml --all > results_linux.xml
                            '''
                        }
                    }
                    post {
                        always {
                            junit 'tests/results_linux.xml'
                            archiveArtifacts artifacts: 'tests/*.log',
                                           allowEmptyArchive: true
                        }
                    }
                }
            }
        }

        stage('Analyse des r√©sultats') {
            agent any
            steps {
                script {
                    echo 'G√©n√©ration du rapport consolid√©'
                    // Script pour merger les r√©sultats
                }
            }
        }
    }

    post {
        failure {
            emailext (
                subject: "Tests √©chou√©s: ${env.JOB_NAME} - ${env.BUILD_NUMBER}",
                body: "Les tests ont √©chou√©. Voir ${env.BUILD_URL}",
                to: 'equipe-dev@exemple.com'
            )
        }
        success {
            echo 'Tous les tests sont pass√©s!'
        }
    }
}
```

## G√©n√©rateur de rapports HTML complet

### Classe de g√©n√©ration de rapports avanc√©e

```pascal
unit ReportGenerator;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, FPCUnit, TestReport, fpjson, jsonparser;

type
  TTestReportGenerator = class
  private
    FResults: TTestResult;
    FPlatform: string;
    FOutputDir: string;

    function GetPlatformInfo: TJSONObject;
    function GetTestResultsJSON: TJSONArray;
    procedure GenerateCSS(const AFileName: string);
    procedure GenerateJS(const AFileName: string);
    procedure GenerateCharts(const AFileName: string);
  public
    constructor Create(AResults: TTestResult; const AOutputDir: string);
    procedure GenerateFullReport;
    procedure GenerateJSONReport(const AFileName: string);
    procedure GenerateSummary(const AFileName: string);
    procedure MergeReports(const AReportFiles: array of string;
                          const AOutputFile: string);
  end;

implementation

constructor TTestReportGenerator.Create(AResults: TTestResult;
                                       const AOutputDir: string);
begin
  FResults := AResults;
  FOutputDir := AOutputDir;

  {$IFDEF WINDOWS}
    FPlatform := 'Windows';
  {$ELSE}
    FPlatform := 'Linux/Unix';
  {$ENDIF}

  ForceDirectories(FOutputDir);
end;

function TTestReportGenerator.GetPlatformInfo: TJSONObject;
begin
  Result := TJSONObject.Create;
  Result.Add('platform', FPlatform);
  Result.Add('fpc_version', {$I %FPCVERSION%});
  Result.Add('date', DateTimeToStr(Now));
  Result.Add('cpu', {$I %FPCTARGETCPU%});
  Result.Add('os', {$I %FPCTARGETOS%});
end;

function TTestReportGenerator.GetTestResultsJSON: TJSONArray;
var
  i: Integer;
  TestObj: TJSONObject;
begin
  Result := TJSONArray.Create;

  // Ajouter chaque r√©sultat de test
  for i := 0 to FResults.Testsuite.Tests.Count - 1 do
  begin
    TestObj := TJSONObject.Create;
    TestObj.Add('name', FResults.Testsuite.Tests[i].TestName);
    TestObj.Add('passed', FResults.Testsuite.Tests[i].Passed);
    TestObj.Add('time', FResults.Testsuite.Tests[i].ElapsedTime);

    if not FResults.Testsuite.Tests[i].Passed then
      TestObj.Add('error', FResults.Testsuite.Tests[i].ErrorMessage);

    Result.Add(TestObj);
  end;
end;

procedure TTestReportGenerator.GenerateCSS(const AFileName: string);
var
  F: TextFile;
begin
  AssignFile(F, AFileName);
  Rewrite(F);
  try
    WriteLn(F, '/* Styles pour le rapport de tests */');
    WriteLn(F, 'body {');
    WriteLn(F, '    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;');
    WriteLn(F, '    margin: 0;');
    WriteLn(F, '    padding: 20px;');
    WriteLn(F, '    background: #f5f5f5;');
    WriteLn(F, '}');
    WriteLn(F, '.container {');
    WriteLn(F, '    max-width: 1200px;');
    WriteLn(F, '    margin: 0 auto;');
    WriteLn(F, '    background: white;');
    WriteLn(F, '    border-radius: 8px;');
    WriteLn(F, '    box-shadow: 0 2px 4px rgba(0,0,0,0.1);');
    WriteLn(F, '    padding: 30px;');
    WriteLn(F, '}');
    WriteLn(F, '.header {');
    WriteLn(F, '    border-bottom: 2px solid #e0e0e0;');
    WriteLn(F, '    padding-bottom: 20px;');
    WriteLn(F, '    margin-bottom: 30px;');
    WriteLn(F, '}');
    WriteLn(F, '.platform-badge {');
    WriteLn(F, '    display: inline-block;');
    WriteLn(F, '    padding: 5px 15px;');
    WriteLn(F, '    border-radius: 20px;');
    WriteLn(F, '    font-size: 14px;');
    WriteLn(F, '    font-weight: bold;');
    WriteLn(F, '}');
    WriteLn(F, '.platform-windows { background: #0078d4; color: white; }');
    WriteLn(F, '.platform-linux { background: #dd4814; color: white; }');
    WriteLn(F, '.stats {');
    WriteLn(F, '    display: grid;');
    WriteLn(F, '    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));');
    WriteLn(F, '    gap: 20px;');
    WriteLn(F, '    margin: 30px 0;');
    WriteLn(F, '}');
    WriteLn(F, '.stat-card {');
    WriteLn(F, '    padding: 20px;');
    WriteLn(F, '    border-radius: 8px;');
    WriteLn(F, '    text-align: center;');
    WriteLn(F, '}');
    WriteLn(F, '.stat-value {');
    WriteLn(F, '    font-size: 36px;');
    WriteLn(F, '    font-weight: bold;');
    WriteLn(F, '    margin: 10px 0;');
    WriteLn(F, '}');
    WriteLn(F, '.stat-label {');
    WriteLn(F, '    color: #666;');
    WriteLn(F, '    font-size: 14px;');
    WriteLn(F, '}');
    WriteLn(F, '.success { background: #d4edda; color: #155724; }');
    WriteLn(F, '.failure { background: #f8d7da; color: #721c24; }');
    WriteLn(F, '.warning { background: #fff3cd; color: #856404; }');
    WriteLn(F, '.test-list {');
    WriteLn(F, '    margin-top: 30px;');
    WriteLn(F, '}');
    WriteLn(F, '.test-item {');
    WriteLn(F, '    padding: 15px;');
    WriteLn(F, '    margin: 10px 0;');
    WriteLn(F, '    border-left: 4px solid #ddd;');
    WriteLn(F, '    background: #fafafa;');
    WriteLn(F, '}');
    WriteLn(F, '.test-passed { border-color: #28a745; }');
    WriteLn(F, '.test-failed { border-color: #dc3545; }');
    WriteLn(F, '.test-name { font-weight: bold; }');
    WriteLn(F, '.test-time { color: #666; font-size: 12px; }');
    WriteLn(F, '.error-message {');
    WriteLn(F, '    margin-top: 10px;');
    WriteLn(F, '    padding: 10px;');
    WriteLn(F, '    background: #fff;');
    WriteLn(F, '    border-radius: 4px;');
    WriteLn(F, '    font-family: monospace;');
    WriteLn(F, '    font-size: 12px;');
    WriteLn(F, '}');
  finally
    CloseFile(F);
  end;
end;

procedure TTestReportGenerator.GenerateJS(const AFileName: string);
var
  F: TextFile;
begin
  AssignFile(F, AFileName);
  Rewrite(F);
  try
    WriteLn(F, '// JavaScript pour le rapport de tests');
    WriteLn(F, 'document.addEventListener("DOMContentLoaded", function() {');
    WriteLn(F, '    // Filtrage des tests');
    WriteLn(F, '    const filterButtons = document.querySelectorAll(".filter-btn");');
    WriteLn(F, '    const testItems = document.querySelectorAll(".test-item");');
    WriteLn(F, '    ');
    WriteLn(F, '    filterButtons.forEach(btn => {');
    WriteLn(F, '        btn.addEventListener("click", function() {');
    WriteLn(F, '            const filter = this.dataset.filter;');
    WriteLn(F, '            ');
    WriteLn(F, '            filterButtons.forEach(b => b.classList.remove("active"));');
    WriteLn(F, '            this.classList.add("active");');
    WriteLn(F, '            ');
    WriteLn(F, '            testItems.forEach(item => {');
    WriteLn(F, '                if (filter === "all") {');
    WriteLn(F, '                    item.style.display = "block";');
    WriteLn(F, '                } else if (filter === "passed" && item.classList.contains("test-passed")) {');
    WriteLn(F, '                    item.style.display = "block";');
    WriteLn(F, '                } else if (filter === "failed" && item.classList.contains("test-failed")) {');
    WriteLn(F, '                    item.style.display = "block";');
    WriteLn(F, '                } else {');
    WriteLn(F, '                    item.style.display = "none";');
    WriteLn(F, '                }');
    WriteLn(F, '            });');
    WriteLn(F, '        });');
    WriteLn(F, '    });');
    WriteLn(F, '    ');
    WriteLn(F, '    // Temps d''ex√©cution total');
    WriteLn(F, '    const times = document.querySelectorAll(".test-time");');
    WriteLn(F, '    let totalTime = 0;');
    WriteLn(F, '    times.forEach(t => {');
    WriteLn(F, '        totalTime += parseFloat(t.dataset.time || 0);');
    WriteLn(F, '    });');
    WriteLn(F, '    document.getElementById("total-time").textContent = totalTime.toFixed(2) + " ms";');
    WriteLn(F, '});');
  finally
    CloseFile(F);
  end;
end;

procedure TTestReportGenerator.GenerateFullReport;
var
  F: TextFile;
  i: Integer;
  PassedCount, FailedCount: Integer;
  HTMLFile, CSSFile, JSFile: string;
begin
  HTMLFile := FOutputDir + PathDelim + 'report.html';
  CSSFile := FOutputDir + PathDelim + 'report.css';
  JSFile := FOutputDir + PathDelim + 'report.js';

  // G√©n√©rer les fichiers CSS et JS
  GenerateCSS(CSSFile);
  GenerateJS(JSFile);

  // Calculer les statistiques
  PassedCount := 0;
  FailedCount := 0;
  for i := 0 to FResults.Count - 1 do
  begin
    if FResults.Test[i].TestPassed then
      Inc(PassedCount)
    else
      Inc(FailedCount);
  end;

  // G√©n√©rer le HTML principal
  AssignFile(F, HTMLFile);
  Rewrite(F);
  try
    WriteLn(F, '<!DOCTYPE html>');
    WriteLn(F, '<html lang="fr">');
    WriteLn(F, '<head>');
    WriteLn(F, '    <meta charset="UTF-8">');
    WriteLn(F, '    <meta name="viewport" content="width=device-width, initial-scale=1.0">');
    WriteLn(F, '    <title>Rapport de Tests - ', FPlatform, '</title>');
    WriteLn(F, '    <link rel="stylesheet" href="report.css">');
    WriteLn(F, '</head>');
    WriteLn(F, '<body>');
    WriteLn(F, '    <div class="container">');

    // En-t√™te
    WriteLn(F, '        <div class="header">');
    WriteLn(F, '            <h1>Rapport de Tests Multi-plateformes</h1>');
    WriteLn(F, '            <span class="platform-badge platform-',
            LowerCase(StringReplace(FPlatform, '/', '-', [rfReplaceAll])), '">',
            FPlatform, '</span>');
    WriteLn(F, '            <p>G√©n√©r√© le ', DateTimeToStr(Now), '</p>');
    WriteLn(F, '        </div>');

    // Statistiques
    WriteLn(F, '        <div class="stats">');
    WriteLn(F, '            <div class="stat-card">');
    WriteLn(F, '                <div class="stat-label">Tests totaux</div>');
    WriteLn(F, '                <div class="stat-value">', FResults.Count, '</div>');
    WriteLn(F, '            </div>');
    WriteLn(F, '            <div class="stat-card success">');
    WriteLn(F, '                <div class="stat-label">R√©ussis</div>');
    WriteLn(F, '                <div class="stat-value">', PassedCount, '</div>');
    WriteLn(F, '            </div>');
    WriteLn(F, '            <div class="stat-card failure">');
    WriteLn(F, '                <div class="stat-label">√âchecs</div>');
    WriteLn(F, '                <div class="stat-value">', FailedCount, '</div>');
    WriteLn(F, '            </div>');
    WriteLn(F, '            <div class="stat-card">');
    WriteLn(F, '                <div class="stat-label">Temps total</div>');
    WriteLn(F, '                <div class="stat-value" id="total-time">-</div>');
    WriteLn(F, '            </div>');
    WriteLn(F, '        </div>');

    // Filtres
    WriteLn(F, '        <div class="filters">');
    WriteLn(F, '            <button class="filter-btn active" data-filter="all">Tous</button>');
    WriteLn(F, '            <button class="filter-btn" data-filter="passed">R√©ussis</button>');
    WriteLn(F, '            <button class="filter-btn" data-filter="failed">√âchecs</button>');
    WriteLn(F, '        </div>');

    // Liste des tests
    WriteLn(F, '        <div class="test-list">');
    WriteLn(F, '            <h2>D√©tails des tests</h2>');

    for i := 0 to FResults.Count - 1 do
    begin
      if FResults.Test[i].TestPassed then
        WriteLn(F, '            <div class="test-item test-passed">')
      else
        WriteLn(F, '            <div class="test-item test-failed">');

      WriteLn(F, '                <div class="test-name">',
              FResults.Test[i].TestSuiteName, '::',
              FResults.Test[i].TestName, '</div>');
      WriteLn(F, '                <div class="test-time" data-time="',
              FResults.Test[i].ElapsedTime, '">',
              'Temps: ', FResults.Test[i].ElapsedTime, ' ms</div>');

      if not FResults.Test[i].TestPassed then
      begin
        WriteLn(F, '                <div class="error-message">');
        WriteLn(F, FResults.Test[i].Message);
        WriteLn(F, '                </div>');
      end;

      WriteLn(F, '            </div>');
    end;

    WriteLn(F, '        </div>');
    WriteLn(F, '    </div>');
    WriteLn(F, '    <script src="report.js"></script>');
    WriteLn(F, '</body>');
    WriteLn(F, '</html>');
  finally
    CloseFile(F);
  end;

  WriteLn('Rapport g√©n√©r√©: ', HTMLFile);
end;

procedure TTestReportGenerator.GenerateJSONReport(const AFileName: string);
var
  RootObj: TJSONObject;
  ResultsArray: TJSONArray;
begin
  RootObj := TJSONObject.Create;
  try
    RootObj.Add('platform_info', GetPlatformInfo);
    RootObj.Add('summary', TJSONObject.Create);
    RootObj.Objects['summary'].Add('total', FResults.Count);
    RootObj.Objects['summary'].Add('passed', FResults.NumberOfRuns -
                                             FResults.NumberOfFailures);
    RootObj.Objects['summary'].Add('failed', FResults.NumberOfFailures);
    RootObj.Objects['summary'].Add('errors', FResults.NumberOfErrors);

    RootObj.Add('tests', GetTestResultsJSON);

    // Sauvegarder le JSON
    with TStringList.Create do
    try
      Text := RootObj.FormatJSON;
      SaveToFile(AFileName);
    finally
      Free;
    end;
  finally
    RootObj.Free;
  end;
end;

procedure TTestReportGenerator.GenerateSummary(const AFileName: string);
var
  F: TextFile;
begin
  AssignFile(F, AFileName);
  Rewrite(F);
  try
    WriteLn(F, 'R√âSUM√â DES TESTS');
    WriteLn(F, '================');
    WriteLn(F);
    WriteLn(F, 'Plateforme: ', FPlatform);
    WriteLn(F, 'Date: ', DateTimeToStr(Now));
    WriteLn(F);
    WriteLn(F, 'R√©sultats:');
    WriteLn(F, '  Total: ', FResults.Count);
    WriteLn(F, '  R√©ussis: ', FResults.Count - FResults.NumberOfFailures);
    WriteLn(F, '  √âchecs: ', FResults.NumberOfFailures);
    WriteLn(F, '  Erreurs: ', FResults.NumberOfErrors);
    WriteLn(F);

    if FResults.NumberOfFailures > 0 then
    begin
      WriteLn(F, 'Tests √©chou√©s:');
      // Lister les tests √©chou√©s
    end;
  finally
    CloseFile(F);
  end;
end;

procedure TTestReportGenerator.MergeReports(const AReportFiles: array of string;
                                           const AOutputFile: string);
begin
  // Fusionner plusieurs rapports JSON en un seul
  // Utile pour consolider les r√©sultats Windows et Linux
end;

end.
```

## Conclusion et recommandations finales

### Checklist pour tests cross-platform r√©ussis

1. **Organisation du code**
   - S√©parer les tests unitaires, d'int√©gration et de performance
   - Un fichier de test par module de code
   - Utiliser des noms de tests descriptifs

2. **Gestion des diff√©rences OS**
   - Utiliser la compilation conditionnelle intelligemment
   - Abstraire les op√©rations syst√®me dans des interfaces
   - Tester les chemins, permissions et comportements sp√©cifiques

3. **Automatisation**
   - Scripts de build pour chaque plateforme
   - Int√©gration CI/CD (GitHub Actions, Jenkins, GitLab CI)
   - Rapports automatiques apr√®s chaque commit

4. **Performance**
   - Benchmarks r√©guliers
   - Comparaison avec des baselines
   - Surveillance des r√©gressions de performance
   - Tests de charge adapt√©s √† chaque OS
   - Optimisation bas√©e sur les m√©triques r√©elles

5. **Qualit√© des tests**
   - Viser une couverture de code > 80%
   - Tests ind√©pendants et r√©p√©tables
   - Nettoyage syst√©matique apr√®s chaque test
   - Documentation des cas de test complexes

6. **Maintenance**
   - Mise √† jour r√©guli√®re des snapshots
   - R√©vision p√©riodique des tests obsol√®tes
   - Adaptation aux nouvelles versions d'OS
   - Archivage des rapports pour suivi historique

## Structure recommand√©e d'un projet avec tests

### Organisation compl√®te des dossiers

```
MonProjet/
‚îú‚îÄ‚îÄ src/                          # Code source principal
‚îÇ   ‚îú‚îÄ‚îÄ core/                     # Logique m√©tier
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ ui/                       # Interface utilisateur
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ forms/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components/
‚îÇ   ‚îî‚îÄ‚îÄ platform/                 # Code sp√©cifique OS
‚îÇ       ‚îú‚îÄ‚îÄ windows/
‚îÇ       ‚îî‚îÄ‚îÄ linux/
‚îú‚îÄ‚îÄ tests/                        # Tous les tests
‚îÇ   ‚îú‚îÄ‚îÄ unit/                     # Tests unitaires
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ integration/              # Tests d'int√©gration
‚îÇ   ‚îú‚îÄ‚îÄ performance/              # Tests de performance
‚îÇ   ‚îú‚îÄ‚îÄ fixtures/                 # Donn√©es de test
‚îÇ   ‚îú‚îÄ‚îÄ mocks/                    # Mock objects
‚îÇ   ‚îú‚îÄ‚îÄ snapshots/                # Snapshots pour r√©gression
‚îÇ   ‚îî‚îÄ‚îÄ reports/                  # Rapports g√©n√©r√©s
‚îú‚îÄ‚îÄ scripts/                      # Scripts d'automatisation
‚îÇ   ‚îú‚îÄ‚îÄ build_windows.bat
‚îÇ   ‚îú‚îÄ‚îÄ build_linux.sh
‚îÇ   ‚îú‚îÄ‚îÄ test_windows.bat
‚îÇ   ‚îú‚îÄ‚îÄ test_linux.sh
‚îÇ   ‚îî‚îÄ‚îÄ deploy.sh
‚îú‚îÄ‚îÄ ci/                          # Configuration CI/CD
‚îÇ   ‚îú‚îÄ‚îÄ Jenkinsfile
‚îÇ   ‚îú‚îÄ‚îÄ .github/workflows/
‚îÇ   ‚îî‚îÄ‚îÄ .gitlab-ci.yml
‚îú‚îÄ‚îÄ docs/                        # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ tests/                   # Documentation des tests
‚îî‚îÄ‚îÄ README.md
```

## Script ma√Ætre pour l'ex√©cution compl√®te des tests

### Script Python pour orchestration multi-plateforme

```python
#!/usr/bin/env python3
"""
test_runner.py - Orchestrateur de tests multi-plateformes
"""

import os
import sys
import platform
import subprocess
import json
import datetime
from pathlib import Path

class TestOrchestrator:
    def __init__(self):
        self.platform = platform.system()
        self.project_root = Path(__file__).parent.parent
        self.test_dir = self.project_root / 'tests'
        self.report_dir = self.test_dir / 'reports'
        self.results = []

        # Cr√©er le dossier de rapports
        self.report_dir.mkdir(exist_ok=True)

    def compile_tests(self):
        """Compile les tests pour la plateforme actuelle"""
        print(f"üî® Compilation des tests pour {self.platform}...")

        compile_cmd = [
            'fpc',
            '-Mobjfpc',
            '-Sh',
            '-B',
            str(self.test_dir / 'TestsConsole.pas')
        ]

        try:
            result = subprocess.run(compile_cmd,
                                  capture_output=True,
                                  text=True)
            if result.returncode != 0:
                print(f"‚ùå Erreur de compilation:\n{result.stderr}")
                return False
            print("‚úÖ Compilation r√©ussie")
            return True
        except Exception as e:
            print(f"‚ùå Erreur: {e}")
            return False

    def run_unit_tests(self):
        """Ex√©cute les tests unitaires"""
        print("\nüìã Ex√©cution des tests unitaires...")
        return self._run_test_suite('unit')

    def run_integration_tests(self):
        """Ex√©cute les tests d'int√©gration"""
        print("\nüîó Ex√©cution des tests d'int√©gration...")
        return self._run_test_suite('integration')

    def run_performance_tests(self):
        """Ex√©cute les tests de performance"""
        print("\n‚ö° Ex√©cution des tests de performance...")
        return self._run_test_suite('performance')

    def _run_test_suite(self, suite_type):
        """Ex√©cute une suite de tests sp√©cifique"""
        if self.platform == 'Windows':
            exe_name = 'TestsConsole.exe'
        else:
            exe_name = './TestsConsole'

        test_cmd = [
            str(self.test_dir / exe_name),
            f'--suite={suite_type}',
            '--format=json',
            '--verbose'
        ]

        try:
            result = subprocess.run(test_cmd,
                                  capture_output=True,
                                  text=True,
                                  cwd=str(self.test_dir))

            # Parser les r√©sultats JSON
            if result.stdout:
                try:
                    test_results = json.loads(result.stdout)
                    self.results.append({
                        'suite': suite_type,
                        'results': test_results,
                        'timestamp': datetime.datetime.now().isoformat()
                    })

                    # Afficher le r√©sum√©
                    passed = test_results.get('passed', 0)
                    failed = test_results.get('failed', 0)
                    total = passed + failed

                    if failed == 0:
                        print(f"  ‚úÖ {passed}/{total} tests r√©ussis")
                    else:
                        print(f"  ‚ùå {failed}/{total} tests √©chou√©s")

                    return failed == 0
                except json.JSONDecodeError:
                    print(f"  ‚ö†Ô∏è Impossible de parser les r√©sultats")
                    return False

            return result.returncode == 0

        except Exception as e:
            print(f"  ‚ùå Erreur: {e}")
            return False

    def generate_report(self):
        """G√©n√®re un rapport HTML consolid√©"""
        print("\nüìä G√©n√©ration du rapport...")

        report_file = self.report_dir / f"report_{self.platform.lower()}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.html"

        html_content = self._generate_html_report()

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f"‚úÖ Rapport g√©n√©r√©: {report_file}")
        return report_file

    def _generate_html_report(self):
        """G√©n√®re le contenu HTML du rapport"""
        total_passed = sum(r['results'].get('passed', 0) for r in self.results)
        total_failed = sum(r['results'].get('failed', 0) for r in self.results)

        html = f"""<!DOCTYPE html>
<html>
<head>
    <title>Rapport de Tests - {self.platform}</title>
    <meta charset="UTF-8">
    <style>
        body {{
            font-family: Arial, sans-serif;
            margin: 40px;
            background: #f5f5f5;
        }}
        .header {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        h1 {{
            color: #333;
            margin: 0;
        }}
        .summary {{
            display: flex;
            gap: 20px;
            margin-top: 20px;
        }}
        .summary-card {{
            flex: 1;
            padding: 15px;
            border-radius: 8px;
            text-align: center;
        }}
        .passed {{ background: #d4edda; color: #155724; }}
        .failed {{ background: #f8d7da; color: #721c24; }}
        .total {{ background: #cce5ff; color: #004085; }}
        .suite-section {{
            background: white;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        table {{
            width: 100%;
            border-collapse: collapse;
            margin-top: 15px;
        }}
        th, td {{
            padding: 10px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }}
        th {{
            background: #f8f9fa;
            font-weight: bold;
        }}
        .test-passed {{ color: green; }}
        .test-failed {{ color: red; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>Rapport de Tests Multi-plateformes</h1>
        <p>Plateforme: <strong>{self.platform}</strong></p>
        <p>Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>

        <div class="summary">
            <div class="summary-card total">
                <h3>Total</h3>
                <p style="font-size: 24px; font-weight: bold;">{total_passed + total_failed}</p>
            </div>
            <div class="summary-card passed">
                <h3>R√©ussis</h3>
                <p style="font-size: 24px; font-weight: bold;">{total_passed}</p>
            </div>
            <div class="summary-card failed">
                <h3>√âchecs</h3>
                <p style="font-size: 24px; font-weight: bold;">{total_failed}</p>
            </div>
        </div>
    </div>
"""

        for result in self.results:
            suite_name = result['suite'].capitalize()
            suite_results = result['results']

            html += f"""
    <div class="suite-section">
        <h2>Tests {suite_name}</h2>
        <table>
            <thead>
                <tr>
                    <th>Test</th>
                    <th>Statut</th>
                    <th>Temps (ms)</th>
                    <th>Message</th>
                </tr>
            </thead>
            <tbody>
"""

            # Ajouter les d√©tails des tests (si disponibles)
            if 'tests' in suite_results:
                for test in suite_results['tests']:
                    status_class = 'test-passed' if test.get('passed') else 'test-failed'
                    status_text = '‚úÖ R√©ussi' if test.get('passed') else '‚ùå √âchou√©'

                    html += f"""
                <tr>
                    <td>{test.get('name', 'N/A')}</td>
                    <td class="{status_class}">{status_text}</td>
                    <td>{test.get('time', 'N/A')}</td>
                    <td>{test.get('message', '-')}</td>
                </tr>
"""

            html += """
            </tbody>
        </table>
    </div>
"""

        html += """
</body>
</html>
"""
        return html

    def run_all(self):
        """Ex√©cute tous les tests et g√©n√®re le rapport"""
        print(f"üöÄ D√©marrage des tests sur {self.platform}")
        print("=" * 50)

        # Compilation
        if not self.compile_tests():
            print("\n‚ùå √âchec de la compilation. Arr√™t.")
            return False

        # Ex√©cution des diff√©rentes suites
        all_passed = True
        all_passed &= self.run_unit_tests()
        all_passed &= self.run_integration_tests()
        all_passed &= self.run_performance_tests()

        # G√©n√©ration du rapport
        report_file = self.generate_report()

        # R√©sum√© final
        print("\n" + "=" * 50)
        if all_passed:
            print("‚úÖ TOUS LES TESTS SONT PASS√âS!")
        else:
            print("‚ùå DES TESTS ONT √âCHOU√â")

        print(f"\nüìÑ Rapport disponible: {report_file}")

        # Ouvrir le rapport dans le navigateur (optionnel)
        if platform.system() == 'Windows':
            os.startfile(report_file)
        elif platform.system() == 'Darwin':  # macOS
            subprocess.run(['open', report_file])
        elif platform.system() == 'Linux':
            subprocess.run(['xdg-open', report_file])

        return all_passed


def main():
    orchestrator = TestOrchestrator()
    success = orchestrator.run_all()

    # Code de sortie pour CI/CD
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()
```

## Makefile pour automatisation compl√®te

### Makefile multi-plateforme

```makefile
# Makefile pour tests cross-platform FreePascal/Lazarus

# D√©tection de l'OS
ifeq ($(OS),Windows_NT)
    PLATFORM := Windows
    EXE_EXT := .exe
    RM := del /Q
    MKDIR := mkdir
    PATH_SEP := \\
else
    PLATFORM := $(shell uname -s)
    EXE_EXT :=
    RM := rm -f
    MKDIR := mkdir -p
    PATH_SEP := /
endif

# Configuration
FPC := fpc
FPC_FLAGS := -Mobjfpc -Sh -B -FE./bin -FU./units
TEST_DIR := tests
BIN_DIR := bin
REPORT_DIR := $(TEST_DIR)$(PATH_SEP)reports

# Fichiers
TEST_CONSOLE := $(BIN_DIR)$(PATH_SEP)TestsConsole$(EXE_EXT)
TEST_GUI := $(BIN_DIR)$(PATH_SEP)TestsGUI$(EXE_EXT)

# Cibles principales
.PHONY: all clean test test-console test-gui test-unit test-integration \
        test-performance report help

all: test-console test-gui

help:
	@echo "Commandes disponibles:"
	@echo "  make all              - Compile tous les tests"
	@echo "  make test             - Ex√©cute tous les tests"
	@echo "  make test-unit        - Ex√©cute les tests unitaires"
	@echo "  make test-integration - Ex√©cute les tests d'int√©gration"
	@echo "  make test-performance - Ex√©cute les tests de performance"
	@echo "  make report           - G√©n√®re le rapport HTML"
	@echo "  make clean            - Nettoie les fichiers g√©n√©r√©s"
	@echo "  make ci               - Mode CI/CD (tous les tests + rapport)"

# Cr√©ation des dossiers
$(BIN_DIR):
	$(MKDIR) $(BIN_DIR)

$(REPORT_DIR):
	$(MKDIR) $(REPORT_DIR)

# Compilation
test-console: $(BIN_DIR)
	$(FPC) $(FPC_FLAGS) $(TEST_DIR)$(PATH_SEP)TestsConsole.pas

test-gui: $(BIN_DIR)
	$(FPC) $(FPC_FLAGS) $(TEST_DIR)$(PATH_SEP)TestsGUI.pas

# Ex√©cution des tests
test: test-console
	@echo "========================================="
	@echo "Ex√©cution des tests sur $(PLATFORM)"
	@echo "========================================="
	$(TEST_CONSOLE) --all --format=plain

test-unit: test-console
	@echo "Tests unitaires..."
	$(TEST_CONSOLE) --suite=unit --format=xml > $(REPORT_DIR)$(PATH_SEP)unit.xml

test-integration: test-console
	@echo "Tests d'int√©gration..."
	$(TEST_CONSOLE) --suite=integration --format=xml > $(REPORT_DIR)$(PATH_SEP)integration.xml

test-performance: test-console
	@echo "Tests de performance..."
	$(TEST_CONSOLE) --suite=performance --format=xml > $(REPORT_DIR)$(PATH_SEP)performance.xml

# G√©n√©ration du rapport
report: $(REPORT_DIR) test-unit test-integration test-performance
	@echo "G√©n√©ration du rapport..."
	python3 scripts/generate_report.py

# Mode CI/CD
ci: clean all test report
	@echo "========================================="
	@echo "Build CI/CD termin√©"
	@echo "Plateforme: $(PLATFORM)"
	@echo "========================================="

# Nettoyage
clean:
	$(RM) $(BIN_DIR)$(PATH_SEP)*$(EXE_EXT)
	$(RM) $(TEST_DIR)$(PATH_SEP)*.ppu
	$(RM) $(TEST_DIR)$(PATH_SEP)*.o
	$(RM) $(REPORT_DIR)$(PATH_SEP)*.*

# Installation des d√©pendances (pour CI/CD)
install-deps:
ifeq ($(PLATFORM),Windows)
	@echo "Installation des d√©pendances Windows..."
	# Commandes sp√©cifiques Windows
else
	@echo "Installation des d√©pendances Linux/Unix..."
	sudo apt-get update
	sudo apt-get install -y fpc lazarus
endif
```

## Mod√®le de documentation des tests

### Template pour documenter les tests

```markdown
# Documentation des Tests - [Nom du Module]

## Vue d'ensemble
Description du module test√© et de la strat√©gie de test adopt√©e.

## Tests unitaires

### Test: `TestCalculateurTaxe`
**Objectif:** V√©rifier le calcul correct des taxes selon les r√©gions

**Cas de test:**
- Calcul avec taux standard (20%)
- Calcul avec taux r√©duit (5.5%)
- Gestion des arrondis
- Cas limites (montant n√©gatif, z√©ro)

**Sp√©cificit√©s cross-platform:**
- Windows: Utilise les param√®tres r√©gionaux syst√®me
- Linux: Utilise la locale configur√©e

**Donn√©es de test:**
```
Montant: 100.00 EUR
Taux: 20%
R√©sultat attendu: 120.00 EUR
```

### Test: `TestGestionFichiers`
**Objectif:** V√©rifier les op√©rations sur les fichiers

**Diff√©rences OS:**
| Op√©ration | Windows | Linux |
|-----------|---------|-------|
| Chemin temp | %TEMP% | /tmp |
| S√©parateur | \ | / |
| Permissions | Attributs | chmod |

## Tests d'int√©gration

### Test: `TestConnexionBDD`
**Objectif:** V√©rifier la connexion √† diff√©rentes bases de donn√©es

**Configuration:**
- Windows: SQL Server, PostgreSQL via ODBC
- Linux: PostgreSQL natif, MySQL

**Timeout:** 30 secondes

## Tests de performance

### Benchmark: `BenchmarkTriListe`
**M√©triques cibles:**
| Plateforme | Temps moyen | Temps max |
|------------|-------------|-----------|
| Windows | < 50ms | < 100ms |
| Linux | < 40ms | < 80ms |

**Volume de donn√©es:** 10,000 √©l√©ments

## Probl√®mes connus

### Windows
- Les tests de permissions peuvent √©chouer sans droits admin
- Antivirus peut ralentir les tests I/O

### Linux
- Tests GUI n√©cessitent un serveur X
- SELinux peut bloquer certaines op√©rations

## Maintenance

**Derni√®re mise √† jour:** [Date]  
**Responsable:** [Nom]  
**Fr√©quence de r√©vision:** Mensuelle  
```

## Ressources et r√©f√©rences utiles

### Documentation officielle
- [FPCUnit Documentation](https://wiki.freepascal.org/FPCUnit)
- [Lazarus Testing](https://wiki.lazarus.freepascal.org/Testing)
- [FreePascal Cross-compilation](https://wiki.freepascal.org/Cross_compiling)

### Outils recommand√©s
1. **Couverture de code**: [FPCover](https://wiki.freepascal.org/FPCover)
2. **Analyse statique**: [Pascal Analyzer](http://www.peganza.com)
3. **Profiling Windows**: DProf, Intel VTune
4. **Profiling Linux**: gprof, Valgrind, perf

### Bonnes pratiques √† retenir

1. **Isolement des tests**
   - Chaque test doit √™tre ind√©pendant
   - Utiliser SetUp et TearDown syst√©matiquement
   - Nettoyer toutes les ressources

2. **Nommage coh√©rent**
   - `TestNomClasse` pour les classes de test
   - `TestNomMethode` pour les m√©thodes de test
   - Pr√©fixes clairs: `Bench`, `Mock`, `Stub`

3. **Gestion des erreurs**
   - Toujours tester les cas d'erreur
   - V√©rifier les exceptions attendues
   - Logger les erreurs pour diagnostic

4. **Performance**
   - Benchmarks reproductibles
   - Warm-up avant mesures
   - Plusieurs it√©rations pour moyenne

5. **Documentation**
   - Commenter les tests complexes
   - Documenter les d√©pendances OS
   - Maintenir un changelog des tests

## Exemple d'int√©gration compl√®te

### Programme principal avec tests int√©gr√©s

```pascal
program MonApplicationAvecTests;

{$mode objfpc}{$H+}

uses
  {$IFDEF UNIX}
    cthreads,
  {$ENDIF}
  Classes, SysUtils, Forms, Interfaces,
  // Unit√©s de l'application
  FormPrincipal, MonModule,
  // Tests (seulement en mode debug)
  {$IFDEF DEBUG}
    FPCUnit, TestRegistry, ConsoleTestRunner,
    TestMonModule, TestInterface, TestPlateforme,
  {$ENDIF}
  ;

{$R *.res}

procedure RunTests;
{$IFDEF DEBUG}
var
  TestRunner: TTestRunner;
begin
  WriteLn('Mode DEBUG - Ex√©cution des tests...');

  TestRunner := TTestRunner.Create(nil);
  try
    TestRunner.Initialize;
    TestRunner.Run;

    if TestRunner.HasErrors then
    begin
      WriteLn('‚ùå Des tests ont √©chou√©!');
      Halt(1);
    end
    else
      WriteLn('‚úÖ Tous les tests sont pass√©s');
  finally
    TestRunner.Free;
  end;
end;
{$ELSE}
begin
  // Rien en mode release
end;
{$ENDIF}

begin
  // Ex√©cuter les tests si demand√©
  if (ParamCount > 0) and (ParamStr(1) = '--test') then
  begin
    RunTests;
    Exit;
  end;

  // D√©marrage normal de l'application
  Application.Initialize;
  Application.Title := 'Mon Application';
  Application.CreateForm(TFormPrincipal, FormPrincipal);
  Application.Run;
end.
```

## Conclusion

Les tests cross-platform automatis√©s sont essentiels pour garantir la qualit√© et la fiabilit√© de vos applications FreePascal/Lazarus sur Windows et Linux/Ubuntu. En suivant les pratiques pr√©sent√©es dans ce tutoriel, vous pourrez :

- **D√©tecter rapidement** les probl√®mes sp√©cifiques √† chaque plateforme
- **Automatiser** la validation de votre code
- **Maintenir** une qualit√© constante
- **Documenter** le comportement attendu
- **Mesurer** les performances sur chaque OS

L'investissement initial dans la mise en place d'une infrastructure de tests solide sera rapidement rentabilis√© par la r√©duction des bugs en production et la confiance accrue dans vos d√©ploiements multi-plateformes.

N'oubliez pas que les tests ne sont pas une destination mais un voyage continu. Commencez petit, avec quelques tests critiques, puis √©tendez progressivement votre couverture. L'important est d'avoir une base solide sur laquelle construire et d'int√©grer les tests dans votre flux de d√©veloppement quotidien.

Bon d√©veloppement et bons tests ! üöÄ

‚è≠Ô∏è [Int√©gration native par plateforme](/05-developpement-multiplateforme-approfondi/07-integration-native-par-plateforme.md)
